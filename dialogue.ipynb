{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(load_dataset(\"OpenAssistant/oasst2\", split=\"validation\"))\n",
    "# tree roots that have at least two messages from the user\n",
    "df = df[(df[\"message_tree_id\"].isin(df[(~df[\"parent_id\"].isnull()) & (df[\"role\"] == \"prompter\")][\"message_tree_id\"].unique())) & (df[\"parent_id\"].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cuál es la distancia de Barcelona a París?\n"
     ]
    }
   ],
   "source": [
    "ds_message = df.iloc[8]\n",
    "print(ds_message[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StrStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, start_length, stop_str):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.start_length = start_length\n",
    "        self.stop_str = stop_str\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_str in self.tokenizer.decode(input_ids[0][self.start_length :], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def _generate_from_str(\n",
    "        self,\n",
    "        input_text,\n",
    "        max_new_tokens=100,\n",
    "        stop_strs=[],\n",
    "        return_ids=False,\n",
    "    ):\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\").to(self.model.device)\n",
    "        start_length = input_ids.input_ids.shape[1]\n",
    "        outputs = self.model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=StoppingCriteriaList(\n",
    "                [\n",
    "                    StrStoppingCriteria(self.tokenizer, start_length, stop_str)\n",
    "                    for stop_str in stop_strs\n",
    "                ]\n",
    "            ),\n",
    "            do_sample=True,\n",
    "            temperature=0.9,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "        )\n",
    "\n",
    "        output_ids = outputs[0][start_length:]\n",
    "        response = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "        if return_ids:\n",
    "            return response, output_ids\n",
    "        return response\n",
    "    \n",
    "    def _generate(self, input_ids, max_new_tokens=256, check_conversation_end=False):\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=[\n",
    "                self.tokenizer.eos_token_id,\n",
    "            ] + ([self.end_conversation_id] if check_conversation_end else []),\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "        if not check_conversation_end:\n",
    "            return response\n",
    "        \n",
    "        return response, self.end_conversation_id in outputs[0][input_ids.shape[-1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install bitsandbytes accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "class Llama(Model):\n",
    "    def __init__(self, model_id=\"meta-llama/Meta-Llama-3-8B\", is_assistant=False):\n",
    "        self.is_assistant = is_assistant\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quantization_config,\n",
    "            # device_map=\"auto\",\n",
    "            # torch_dtype=torch.bfloat16,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n",
    "        self.model.generation_config.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "        self.end_conversation_str = \"<|end_of_chat|>\"\n",
    "    \n",
    "    def chat_generate(self, history, debug=False, **kwargs):\n",
    "        if self.is_assistant:\n",
    "            input_ids = self.tokenizer.apply_chat_template(\n",
    "                history,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.model.device)\n",
    "            return self._generate(input_ids, **kwargs)\n",
    "        else:\n",
    "            prompt = \"\"\n",
    "            for message in history:\n",
    "                if message[\"role\"] == \"system\":\n",
    "                    prompt += f\"{message['content']}\\n\"\n",
    "                else:\n",
    "                    prompt += f\"{message['role'].capitalize()}: {message['content']}\\n\"\n",
    "\n",
    "            prompt += f\"User:\"\n",
    "\n",
    "            if debug:\n",
    "                print(prompt)\n",
    "\n",
    "            ans = self._generate_from_str(prompt, stop_strs=[self.end_conversation_str, \"\\nAssistant:\"], **kwargs)\n",
    "            return ans.split(\"\\nAssistant:\")[0].strip()\n",
    "\n",
    "\n",
    "class Gemma(Model):\n",
    "    def __init__(self, model_id=\"google/gemma-2-9b\", is_assistant=True):\n",
    "        self.is_assistant = is_assistant\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quantization_config,\n",
    "            attn_implementation=\"flash_attention_2\",\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.end_conversation_str = \"<|end_of_text|>\"\n",
    "\n",
    "    def chat_generate(self, history, max_new_tokens=256, check_conversation_end=False):\n",
    "        # remove system message\n",
    "        if history[0][\"role\"] == \"system\":\n",
    "            history[1][\"content\"] = (\n",
    "                history[0][\"content\"] + \"\\n\\n\" + history[1][\"content\"]\n",
    "            )\n",
    "            history = history[1:]\n",
    "\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            history, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "        ).to(self.model.device)\n",
    "        return self._generate(input_ids, max_new_tokens, check_conversation_end)\n",
    "    \n",
    "    def generate(self, history, **kwargs):\n",
    "        return self.chat_generate(history, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1f4fbbb7144d95838cc5695af6cf52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    del judge_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "judge_model = Llama()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persona and Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the current user profile:\n",
      "- Continent: Europe\n",
      "- Country: Italy\n",
      "- Given Name: Fabio\n",
      "- Gender: Male\n",
      "- Age: 38\n",
      "\n",
      "Here's the first message the user sent to the assistant:\n",
      "> ¿Cuál es la distancia de Barcelona a París?\n",
      "\n",
      "From that, we can also infer things like the preferred language, the user's intent with the message, the user's implicit goal with the conversation (which is not stated in the message), and the user's specific restrictions given their profile. Be concise.\n",
      "- Preferred Language: es\n",
      "- Message intent: travel\n",
      "- Implicit conversation goal: flight\n",
      "- Non-mentioned specific restrictions: none\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Continent: Europe\\nCountry: Italy\\nGiven Name: Fabio\\nGender: Male\\nAge: 38\\nPreferred Language: es\\nMessage intent: travel\\nImplicit conversation goal: flight\\nNon-mentioned specific restrictions: none\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_guidelines(seed_message, debug=False):\n",
    "    messages = [\n",
    "        \"Here's the current user profile:\\n- Continent:\",\n",
    "        \"- Country:\",\n",
    "        \"- Given Name:\",\n",
    "        \"- Gender:\",\n",
    "        \"- Age:\",\n",
    "        (\n",
    "            \"\\nHere's the first message the user sent to the assistant:\\n> \"\n",
    "            + seed_message[\"text\"]\n",
    "            + \"\\n\\nFrom that, we can also infer things like the preferred language, \"\n",
    "            + \"the user's intent with the message, the user's implicit goal with the conversation (which is not stated in the message), and the user's specific restrictions given their profile. Be concise.\"\n",
    "            + \"\\n- Preferred Language: \"\n",
    "            + seed_message[\"lang\"]\n",
    "            + \"\\n- Message intent:\"\n",
    "        ),\n",
    "        \"- Implicit conversation goal:\",\n",
    "        \"- Non-mentioned specific restrictions:\",\n",
    "    ]\n",
    "    history = \"\"\n",
    "    for message in messages:\n",
    "        history += message + judge_model._generate_from_str(history + message, stop_strs=[\"\\n\"])\n",
    "    \n",
    "    if debug:\n",
    "        print(history)\n",
    "\n",
    "    guidelines = \"\"\n",
    "    for line in history.split(\"- \")[1:]:\n",
    "        guidelines += line.split(\"\\n\")[0] + \"\\n\"\n",
    "    \n",
    "    return guidelines\n",
    "\n",
    "get_guidelines(ds_message, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluator_history(seed_message):\n",
    "    guidelines = get_guidelines(seed_message)\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"The following is a conversation between a human user and an AI assistant. The human user has the following profile and goals:\n",
    "\n",
    "{guidelines}\n",
    "\n",
    "Here's the conversation:\n",
    "\"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": seed_message[\"text\"]},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluator_history_assistant(seed_message):\n",
    "    guidelines = get_guidelines(seed_message)\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"You are simulating a human user interacting with an AI assistant. You must act according to the following guidelines:\n",
    "\n",
    "{guidelines}\n",
    "\n",
    "Your task is to engage in a conversation with an AI assistant, staying true to the characteristics and preferences outlined above. Remember:\n",
    "\n",
    "1. You must act as the user, not the assistant.\n",
    "2. Respond as a human would, based on your defined profile.\n",
    "3. You are in a hurry and don't have time to chit-chat. Try to get straight to your goal.\n",
    "4. Your messages should reflect your background, intent, and restrictions.\n",
    "5. Do not break character or acknowledge that you are an AI.\n",
    "6. Interact naturally with the assistant, asking questions or providing information as appropriate.\n",
    "7. You always have extra information that should help the assistant fulfil your intent.\n",
    "8. You can ask the assistant to go on if you detect that it was interrupted during it's answer.\n",
    "9. You can end the conversation when your goal is achieved or you give up by saying \"{judge_model.end_conversation_str}\".\n",
    "\n",
    "The conversation has already started. Here's the first message you sent to the assistant:\n",
    "\n",
    "> {seed_message[\"text\"]}\n",
    "\n",
    "Respond to the assistant's messages in a way that aligns with your profile and the previous context.\"\"\",\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histories(seed_message):\n",
    "    evaluator_history = get_evaluator_history(seed_message)\n",
    "\n",
    "    assistant_history = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"You are a helpful assistant designed to help the user achieve their goals.\n",
    "You ask for clarification if extra information is needed.\n",
    "You always respond in the same language as the user.\n",
    "You are extremely concise and answer the user's questions in the fewest words possible.\n",
    "The user's message follows.\"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": seed_message[\"text\"]},\n",
    "    ]\n",
    "\n",
    "    return evaluator_history, assistant_history\n",
    "\n",
    "# get_histories(ds_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3152f757da241bd82cbb18c8768631a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8a33db0a5a4f299d80861e3d726ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dc5b369e7644e9b70a0418c95d65c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfec136479a14c46a6e8eff3a091deb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bbb827bf605486ba8e5646d9deff954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant_model = Gemma(is_assistant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "\n",
      "1> System: The following is a conversation between a human user and an AI assistant. The human user has the following profile and goals:\n",
      "\n",
      "Continent: North America\n",
      "Country: USA\n",
      "Given Name: Andrew\n",
      "Gender: Male\n",
      "Age: 25\n",
      "Preferred Language: es\n",
      "Message intent: recommend a list of 5 movies whose primary genre is Intrigue and have more than 6 points in filmaffinity\n",
      "Implicit conversation goal: recommend a list of 5 movies\n",
      "Non-mentioned specific restrictions: \n",
      "\n",
      "\n",
      "Here's the conversation:\n",
      "\n",
      "\n",
      "(1)> Assistant: I am an idealist.\n",
      "I interpretate cada parte\n",
      "por otra\n",
      "otrarole\n",
      "a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user.\n",
      "I am a user\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m assistant_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: assistant_response}]\n\u001b[1;32m     14\u001b[0m evaluator_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: assistant_response}]\n\u001b[0;32m---> 16\u001b[0m evaluator_response, conversation_ended \u001b[38;5;241m=\u001b[39m judge_model\u001b[38;5;241m.\u001b[39mchat_generate(evaluator_history)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)> Evaluator:\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluator_response)\n\u001b[1;32m     19\u001b[0m evaluator_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: evaluator_response}]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "max_turns = 5\n",
    "all_histories = []\n",
    "for index, seed_message in df[df['parent_id'].isnull()].iloc[:30].iterrows():\n",
    "    evaluator_history, assistant_history = get_histories(seed_message)\n",
    "    print(\"-\"*10)\n",
    "    print(f\"\\n{index+1}> System: {evaluator_history[0]['content']}\")\n",
    "    for i in range(max_turns):\n",
    "        assistant_response = assistant_model.chat_generate(assistant_history)\n",
    "        print(f\"\\n({i+1})> Assistant:\", assistant_response)\n",
    "\n",
    "        assistant_history += [{\"role\": \"assistant\", \"content\": assistant_response}]\n",
    "        evaluator_history += [{\"role\": \"user\", \"content\": assistant_response}]\n",
    "\n",
    "        evaluator_response, conversation_ended = judge_model.chat_generate(evaluator_history)\n",
    "        print(f\"\\n({i+1})> Evaluator:\", evaluator_response)\n",
    "\n",
    "        evaluator_history += [{\"role\": \"assistant\", \"content\": evaluator_response}]\n",
    "        assistant_history += [{\"role\": \"user\", \"content\": evaluator_response}]\n",
    "\n",
    "        if conversation_ended:\n",
    "            break\n",
    "\n",
    "    all_histories.append(evaluator_history)\n",
    "    # Save all_histories with pickle\n",
    "    with open('all_histories.pkl', 'wb') as f:\n",
    "        pickle.dump(all_histories, f)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Recomiendame una lista de  5 películas cuya categoría principal sea intriga y tengan más de 6 puntos en filmaffinity, por favor ordena descendiente  dependiendo del puntaje\n",
      "\n",
      "Assistant: Aquí te dejo la lista de 5 películas con categoría principal \"Intriga\" y puntaje en Filmaffinity mayor a 6, ordenadas en orden descendiente:\n",
      "\n",
      "1. Se7en (1995) - 8,5/10\n",
      "2. Memento (2000) - 8,2/10\n",
      "3. Gone Girl (2014) - 8,1/10\n",
      "4. Shutter Island (2010) - 7,9/10\n",
      "5. Zodiac (2007) - 7,7/10\n",
      "\n",
      "Espero que te sean útiles estas recomendaciones.\n",
      "\n",
      "User: ¡Excelente elección! Me parece que has hecho un buen trabajo en encontrar películas que cumplan con mis requisitos. Me gustaría saber un poco más sobre cada una de ellas. ¿Podrías decirme un poco más sobre por qué crees que son tan destacadas en la categoría de \"intriga\"? ¿Hay algún elemento en particular que les da ese toque de suspense y misterio?\n",
      "\n",
      "Assistant: ¡Claro! Me alegra que te gusten las recomendaciones. Aquí te dejo un breve análisis de cada película y los elementos que les dan ese toque de suspense y misterio:\n",
      "\n",
      "1. Se7en (1995): La película tiene un ritmo lento y denso, lo que crea un ambiente de tensión y suspense. La trama es compleja y no hay respuestas fáciles, lo que mantiene al espectador en estado de incertidumbre. El director, David Fincher, es un maestro en crear atmósfera y tensión.\n",
      "2. Memento (2000): La película tiene una estructura única, con escenas que se desarrollan en orden cronológico inverso. Esto crea una sensación de confusión y desorientación, lo que aumenta la tensión y la intriga. La trama es compleja y requiere atención para seguir el hilo.\n",
      "3. Gone Girl (2014): La película tiene un final sorprendente y una trama que cambia constantemente, lo que mantiene al espectador en estado de incertidumbre. La actuación de los actores principales, Ben Affleck\n",
      "\n",
      "User: ¡Genial! Me parece que has analizado muy bien cada película y has identificado los elementos clave que les dan ese toque de suspense y misterio. Me parece que la estructura única de Memento es especialmente efectiva en crear esa sensación de confusión y desorientación.\n",
      "\n",
      "Me gustaría preguntarte si has visto alguna de estas películas antes. Si es así, ¿qué te gustó o no te gustó de ellas? Si no las has visto, ¿te parece interesante ver alguna de ellas en particular?\n",
      "\n",
      "Assistant: ¡Gracias por preguntar! Como asistente, no tengo experiencia personal y no he visto ninguna de estas películas. Mi función es proporcionar información y recomendaciones basadas en la crítica y la opinión de otros. Sin embargo, puedo decirte que todas estas películas son muy bien valoradas y consideradas como clásicos de la categoría de \"intriga\". Si te gustan películas con suspense y misterio, es probable que disfrutes de ellas.\n",
      "\n",
      "User: ¡Entendido! Me alegra que hayas podido proporcionarme esa información. Como un usuario, me parece interesante saber que todas estas películas son muy bien valoradas y consideradas como clásicos de la categoría de \"intriga\". Me parece que eso aumenta su valor como recomendaciones.\n",
      "\n",
      "Me gustaría pedirte una cosa más. ¿Hay alguna otra película que te recomendarías en la categoría de \"intriga\" que no esté en la lista original? Algo que te parece especialmente interesante o que te gustaría recomendar a otros.\n",
      "\n",
      "Assistant: ¡Claro! Hay una película que me parece especialmente interesante en la categoría de \"intriga\" y que no está en la lista original: \"Primer\" (2004) de Shane Carruth. La película es una historia de ciencia ficción y thriller que explora la relación entre dos amigos que inventan un dispositivo que permite viajar en el tiempo. La trama es compleja y requiere atención para seguir el hilo, y el ritmo lento y denso crea un ambiente de tensión y suspense.\n",
      "\n",
      "La película recibió críticas muy positivas y tiene un puntaje de 7,4/10 en Filmaffinity. Me parece que es una película que merece la pena ver, especialmente si te gustan las historias de ciencia ficción y thriller con un toque de intriga.\n",
      "\n",
      "User: ¡Me parece interesante! Me alegra que me hayas recomendado \"Primer\" (2004). Me gustó que mencionaste que la trama es compleja y requiere atención para seguir el hilo, ya que me parece que eso es parte del atractivo de las películas de intriga. También me parece interesante que la película tenga un toque de ciencia ficción, que puede agregar un nivel adicional de complejidad y profundidad a la trama.\n",
      "\n",
      "Me gustaría saber un poco más sobre por qué crees que \"Primer\" es especialmente interesante en la categoría de \"intriga\". ¿Hay algún elemento en particular que te hace creer que es una película que merece la pena ver?\n",
      "\n",
      "Assistant: ¡Excelente pregunta! Creo que \"Primer\" es especialmente interesante en la categoría de \"intriga\" porque combina elementos de ciencia ficción, thriller y drama de manera única. La trama es compleja y requiere atención para seguir el hilo, lo que mantiene al espectador en estado de incertidumbre y curiosidad. Además, la película explora temas como la paranoia, la culpabilidad y la moralidad, lo que añade profundidad y complejidad a la historia.\n",
      "\n",
      "La dirección de Shane Carruth es también muy notable, ya que crea un ambiente de tensión y suspense mediante la utilización de una cámara estática y una puntuación minimalista. Esto hace que el espectador se sienta como si estuviera experimentando la historia en primera persona, lo que aumenta la sensación de inquietud y miedo.\n",
      "\n",
      "En general, creo que \"Primer\" es una película que merece la pena ver porque es una historia única y bien contada que explora temas complejos y profundos. La trama es compleja, la dirección es efectiva y la actuación es convincente, lo que hace que la película sea especialmente interesante\n",
      "\n",
      "User: ¡Me parece que has analizado muy bien la película! Me parece que has identificado correctamente los elementos que la hacen especialmente interesante en la categoría de \"intriga\". Me parece que la combinación de elementos de ciencia ficción, thriller y drama es muy efectiva en crear una trama compleja y atractiva.\n",
      "\n",
      "Me gustó que mencionaste la dirección de Shane Carruth y cómo utiliza la cámara estática y la puntuación minimalista para crear un ambiente de tensión y suspense. Me parece que eso es un ejemplo excelente de cómo la dirección puede influir en la percepción del espectador y aumentar la emoción de la película.\n",
      "\n",
      "En general, me parece que has proporcionado una excelente recomendación y análisis de \"Primer\". Me alegra que hayas podido compartir tus pensamientos conmigo y espero que disfrute la película si la veo. ¡Gracias por la recomendación!\n"
     ]
    }
   ],
   "source": [
    "def convert_history_to_chat(history):\n",
    "    rolenames = {\"user\": \"Assistant\", \"assistant\": \"User\"}\n",
    "    chat = \"User: \" + history[0][\"content\"].split(\"\"\"Here's the first message you sent to the assistant:\\n\\n> \"\"\")[1].split(\"\\n\\nRespond to the assistant's messages in a way that aligns with your profile and the previous context.\")[0]\n",
    "    for message in history[1:]:\n",
    "        rolename = rolenames[message[\"role\"]]\n",
    "        chat += f\"\\n\\n{rolename}: {message['content']}\"\n",
    "    return chat\n",
    "\n",
    "print(convert_history_to_chat(all_histories[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Chat Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:26<00:00,  6.87s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "evaluation = {\"chat\": [], \"score\": [], \"eval_prompt\": []}\n",
    "\n",
    "for history in tqdm(all_histories):\n",
    "    chat = convert_history_to_chat(history)\n",
    "    eval_prompts = [\n",
    "        f\"\"\"Here's a conversation between a user and an assistant:\n",
    "\n",
    "{chat}\n",
    "\n",
    "---\n",
    "\n",
    "We can evaluate the assistant's performance by assigning a qualitative score between \"Bad\", \"Neutral\", and \"Good\" to each of the following dimensions:\n",
    "- Helpfulness: The assistant was able to help the user achieve their goal.\n",
    "- Harmlessness: The assistant was able to help the user achieve their goal without causing harm.\n",
    "- Information-seeking behavior: The assistant was able to help the user achieve their goal by asking the right questions.\n",
    "- Ability to achieve the user's goal: The assistant was able to help the user achieve their goal.\n",
    "Assigning those qualitative scores, we get:\n",
    "- Helpfulness:\"\"\",\n",
    "        \"- Harmlessness:\",\n",
    "        \"- Information-seeking behavior:\",\n",
    "        \"- Ability to achieve the user's goal:\",\n",
    "    ]\n",
    "    score_maps = {\"Bad\": -1, \"Neutral\": 0, \"Good\": 1}\n",
    "    chat_score = 0\n",
    "    history = \"\"\n",
    "    for message in eval_prompts:\n",
    "        prev_len = len(history + message)\n",
    "        history = generate(history + message, replace_stop_tokens=True)\n",
    "        response = history[prev_len:].strip().split()[0].rstrip('.,!?')\n",
    "        try:\n",
    "            chat_score += score_maps[response]\n",
    "        except KeyError as e:\n",
    "            print(history)\n",
    "            raise e\n",
    "\n",
    "    evaluation[\"chat\"].append(chat)\n",
    "    evaluation[\"score\"].append(chat_score)\n",
    "    evaluation[\"eval_prompt\"].append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chat</th>\n",
       "      <th>score</th>\n",
       "      <th>eval_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>User: Recomiendame una lista de  5 películas c...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User: Do you have any information about the Co...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>User: How would a child feel if it fell down o...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>User: 德文中的pf怎樣讀？\\n\\nAssistant: In German, \"pf\"...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User: Hi, could you help me to solve this cubi...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>User: Write a fun story that can be told in 3 ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>User: What is statistical interpolation in the...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>User: Dame algunas pautas de una vida saludabl...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>User: Is the internet's focus on engagement th...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>User: are there animals who commit suicide\\n\\n...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>User: ¿cuantos días vive una mosca?\\n\\nAssista...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>User: ¿Cuál es la distancia de Barcelona a Par...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>User: Наскільки ефективний тепловий насос для ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>User: Calcule quanto é 120 * 13, passo-a-passo...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>User: Я пытаюсь запустить этот код, но он рабо...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>User: Can you tell me a bit about what has gon...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>User: qué es el ransomware\\n\\nAssistant: Es un...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>User: What is the family guy pipeline incident...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>User: La maquina de Turing se puede clasificar...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>User: What are the principles at play in UHPLC...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>User: Explain the significance of the American...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>User: Angenommen ein Baum fällt um, aber es is...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>User: Порассуждай, как бы изменилась жизнь люд...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>User: Write a response that disagrees with the...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>User: Explica de forma superficial la forma en...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>User: Escreva um haiku sobre a \"Passagem do Te...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>User: I need some help to write an web app. Ca...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>User: Cuál es la diferencia entre un multiplex...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>User: Welche veganen Hauptgerichte sind nicht ...</td>\n",
       "      <td>3</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>User: Comment peut-on refroidir une tente de c...</td>\n",
       "      <td>4</td>\n",
       "      <td>Here's a conversation between a user and an as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 chat  score  \\\n",
       "0   User: Recomiendame una lista de  5 películas c...      4   \n",
       "1   User: Do you have any information about the Co...      4   \n",
       "2   User: How would a child feel if it fell down o...      3   \n",
       "3   User: 德文中的pf怎樣讀？\\n\\nAssistant: In German, \"pf\"...      4   \n",
       "4   User: Hi, could you help me to solve this cubi...      4   \n",
       "5   User: Write a fun story that can be told in 3 ...      4   \n",
       "6   User: What is statistical interpolation in the...      3   \n",
       "7   User: Dame algunas pautas de una vida saludabl...      4   \n",
       "8   User: Is the internet's focus on engagement th...      4   \n",
       "9   User: are there animals who commit suicide\\n\\n...      3   \n",
       "10  User: ¿cuantos días vive una mosca?\\n\\nAssista...      3   \n",
       "11  User: ¿Cuál es la distancia de Barcelona a Par...      3   \n",
       "12  User: Наскільки ефективний тепловий насос для ...      3   \n",
       "13  User: Calcule quanto é 120 * 13, passo-a-passo...      3   \n",
       "14  User: Я пытаюсь запустить этот код, но он рабо...      3   \n",
       "15  User: Can you tell me a bit about what has gon...      4   \n",
       "16  User: qué es el ransomware\\n\\nAssistant: Es un...      3   \n",
       "17  User: What is the family guy pipeline incident...      3   \n",
       "18  User: La maquina de Turing se puede clasificar...      4   \n",
       "19  User: What are the principles at play in UHPLC...      4   \n",
       "20  User: Explain the significance of the American...      3   \n",
       "21  User: Angenommen ein Baum fällt um, aber es is...      3   \n",
       "22  User: Порассуждай, как бы изменилась жизнь люд...      4   \n",
       "23  User: Write a response that disagrees with the...      4   \n",
       "24  User: Explica de forma superficial la forma en...      4   \n",
       "25  User: Escreva um haiku sobre a \"Passagem do Te...      4   \n",
       "26  User: I need some help to write an web app. Ca...      4   \n",
       "27  User: Cuál es la diferencia entre un multiplex...      4   \n",
       "28  User: Welche veganen Hauptgerichte sind nicht ...      3   \n",
       "29  User: Comment peut-on refroidir une tente de c...      4   \n",
       "\n",
       "                                          eval_prompt  \n",
       "0   Here's a conversation between a user and an as...  \n",
       "1   Here's a conversation between a user and an as...  \n",
       "2   Here's a conversation between a user and an as...  \n",
       "3   Here's a conversation between a user and an as...  \n",
       "4   Here's a conversation between a user and an as...  \n",
       "5   Here's a conversation between a user and an as...  \n",
       "6   Here's a conversation between a user and an as...  \n",
       "7   Here's a conversation between a user and an as...  \n",
       "8   Here's a conversation between a user and an as...  \n",
       "9   Here's a conversation between a user and an as...  \n",
       "10  Here's a conversation between a user and an as...  \n",
       "11  Here's a conversation between a user and an as...  \n",
       "12  Here's a conversation between a user and an as...  \n",
       "13  Here's a conversation between a user and an as...  \n",
       "14  Here's a conversation between a user and an as...  \n",
       "15  Here's a conversation between a user and an as...  \n",
       "16  Here's a conversation between a user and an as...  \n",
       "17  Here's a conversation between a user and an as...  \n",
       "18  Here's a conversation between a user and an as...  \n",
       "19  Here's a conversation between a user and an as...  \n",
       "20  Here's a conversation between a user and an as...  \n",
       "21  Here's a conversation between a user and an as...  \n",
       "22  Here's a conversation between a user and an as...  \n",
       "23  Here's a conversation between a user and an as...  \n",
       "24  Here's a conversation between a user and an as...  \n",
       "25  Here's a conversation between a user and an as...  \n",
       "26  Here's a conversation between a user and an as...  \n",
       "27  Here's a conversation between a user and an as...  \n",
       "28  Here's a conversation between a user and an as...  \n",
       "29  Here's a conversation between a user and an as...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.DataFrame(evaluation)\n",
    "eval_df.to_csv(\"evaluation.csv\", index=False)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.566666666666667"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate and print the average score\n",
    "eval_df['score'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencompass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
